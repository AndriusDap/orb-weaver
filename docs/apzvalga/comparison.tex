

Straipsnyje lyginami įvairūs metodai skirti svetainių klasifikavimui į kenksmingas ir nekenksmingas. Formuluojama problema yra svetainių klasifikavimas į dvi grupes, naudojant metodus \cite{comp}
\begin{enumerate}
 \item K-artimiausų kaimynų (K-Nearest neighbours);
 \item Atraminių vektorių klasifikatorius (angl. \textit{Support Vector Machine});
 \item Paprastasis Bayes klasifikatorius (angl. \textit{Naive Bayes});
 \item K-vidurkių (K-Means);
 \item Affinity propagation metodas.
\end{enumerate}

Staripsnyje naudojami ir metodai mokomi su mokytoju ir be. Jų rezultatas yra lyginamas siekiant aptikti geriausiai tinkantį parinktai problemai spręsti \cite{comp}.

K-Artimiausų kaimynų (K-Nearest neighbours) metodas yra paremtas trimis elementais \cite{Wu2008}:
\begin{enumerate}
    \item Mokymo duomenų rinkinys;
    \item Elementų atstumo matas;
    \item Analizuojamų kaimynų kiekis $k$.
\end{enumerate}
Elementų klasifikacija vyksta balsavimo principu - išrenkama $k$ elementų, kurie yra artimiausi pagal parinktą artumo metriką, dominuojanti klasė išrinktame rinkinyje priskiriama klasifikuojamam elementui.

Pagrindiniai šio metodo trūkumai yra $k$, kaimynų skaičiaus, parinkimas. Per didelis $k$ lems gretimų klasių įtraukimą į balsavimo procesą, per mažas gali lemti blogą rezultatą esant triukšmui pradiniuose, apmokymo, duomenyse. Tai sprendžiama pridedant balso svorį \cite{Wu2008} kuris yra atvirkščiai proporcingas elemento atstumui nuo balsuojančio elemento. Taip pat svarbus atstumo mato parinkimas. Dažnai naudojamas Euklido atstumas, tačiau jis nėra tinkamas kai yra daug matmenų, ar jų reikšmės yra itin skirtingos \cite{Wu2008}. Tada verta naudoti kitus matus arba normalizuoti atributų vertes.

Šio metodo privalumas yra tai,  kad skaičiavimai atliekami tingiai. Pirminio modelio formavimo metu nėra atliekami jokie skaičiavimai, jie vykdomi tik klasifikuojant naujus duomenis \cite{Wu2008}.

Atraminių vektorių klasifikatoriaus (Support Vector Machine) metodas yra vienas iš patikimiausių klasifikavimo metodų \cite{Wu2008}. Jis pasižymi geru rezultatu net turint mažą kiekį mokymo duomenų bei nenukenčia dėl didelio matmenų kiekio \cite{Wu2008}. Šio metodo esmė yra duomenų atvaizdavimas kitoje erdvėje kurioje bus galima aptikti hiperplokštumą kuri skiria įrašų klases.

Atraminių vektorių klasifikatorius aprašomas\cite{comp}:
\begin{equation}
    h(x) = b + \sum_{n=1}^{N}y_i \alpha_i K(x, x_i),
\end{equation}
kur $h(x)$ yra elemento $x$ atstumas nuo klases skiriančios hiperplokštumos, $b$ yra papildomas svorio koeficientas, $\alpha$ yra hiperplokštumos paraštės korekcijos koeficientas mokymo imčiai, $N$ yra matmenų skaičius, $K$ yra branduolio funkcija kuri transformuoja elementus į atraminių vektorių erdvę, $x$ yra klasifikuojamas įrašas.

Apmokant atraminių vektorių klasifikatorių yra svarbu parinkti tinkamą branduolio funkciją, nuo jos itin priklauso modelio rezultatų tikslumas. Mokymas vykdomas parenkant branduolio funkciją ir pskaičiuojant tokius koeficientus $\alpha_i$ su kuriais gaunamas didžiausias riba tarp hiperplokštumos ir teisingai klasifikuotų mokymo duomenų erdvėje.

Šio metodo trūkumas yra didelio kiekio skaičiavimo resursų reikalavimas modelio apmokymo metu. Tačiau po modelio formavimo jis gali būti panaudojamas nereikalaujant daug resursų \cite{Wu2008}.

Paprastas Bajeso klasifikatorius yra paremtas Bajeso teorema. Laikoma, kad visi įrašų matmenys priklauso nepriklauso vienas nuo kito \cite{comp}. Šio metodo privalumas yra paprastas modelio formavimas, nėra hiperparametrų kuriuos reikėtų derinti. Taip pat jo sudėtingumas mokymosi metu laiko atžvilgiu yra tiesinis, mokymosi imties dydžiui. Tai leidžia jį pritaikyti dideliems duomenų rinkiniams \cite{Wu2008}. Taip pat rezultatas yra pakankamai geras, ypač įvertinant reikalingą pastangų kiekį norint apmokyti modelį \cite{Wu2008}.

  Modelis formuojamas naudojantis Bajeso teorema ir sąlyginėmis tikimybėmis. Jei klasių aibė yra $i = 0, 1$ tik dviejų elementų, $P(i|x)$ yra tikimybė kad įrašas su verčių vektoriumi $x = (x_1, ..., x_p)$ priklauso klasei $i$, tai bet kokia monotoniška $P(i|x)$ gali būti naudojama klasifikavimui \cite{Wu2008}. Ši funkcija gali būti išreiškiama kaip santykis $P(1|x)/P(0|x)$. Tai gali būti išreiškiama kaip \cite{Wu2008}:
    \begin{equation}
        \frac{P(1|x)}{P(0|x)} = \frac{f(x|1)P(1)}{f(x|0)P(0)}
    \end{equation}
  Šią funkciją klasifikavimui galima naudoti radus $f(x|i)$ funkciją. Paprastojo Bajeso klasifikavimo metode laikoma, kad visi $x$ vektorių matmenys yra nepriklausomi, tada $f(x|i)$ prastinama į \cite{Wu2008}
  \begin{equation}
    f(x|i) = \prod_{j=1}^{p} f(x_j|i)
  \end{equation}
  kur visos $f(x_j|i)$ funkcijos yra įvertinamos atskirai, taip supaprastinant problemą į daug  vienmačių uždavinių.

  Šis metodas leidžia panaudoti nedidelį duomenų kiekį modelio apmokymui, apmokymas vyksta greitai ir paprastai, nereikalauja komplikuotų iteracinių schemų modelio formavimui \cite{Wu2008}. Tačiau vienas iš reikalavimų labiausiai ribojančių šį metodą yra visų duomenų matmenų nepriklausomumas tarpusavyje. Tačiau tai galima spręsti naudojant pirminį duomenų apdorojimą, pašalinant stipriai koreliuotas reikšmes \cite{Wu2008}. Taip pat metodas gali modeliuoti tik tiesines ribas tarp klasių, be papildomų modifikacijų jo pritaikyti netiesiniams uždaviniams neina \cite{comp}.

 K-vidurkių algoritmas yra iteracinis metodas skirtas skaidyti duomenų rinkinį į naudotojo parinktą klasterių skaičių $k$ \cite{Wu2008}. Algoritmo metu siekiama minimizuoti visų imties elementų atstumą iki jiems priskirtų klasterių centrų. Algoritmo iteracija susideda iš dviejų žingsnių:

 \textbf{Duomenų priskyrimas.} Visi objektai yra priskiriami jiems artimiausiam centroidui. Esant vienodiems atstumais centroidas parenkamas atsitiktinai. Pirmą kartą vykdant šį žingsnį centroidai parenkami atsitiktinai. Šio žingsnio rezultatas yra duomenys, suskaidyti į $k$ grupių.

 \textbf{Centrų tikslinimas.} Perskaičiuojamos grupių vidutinės parametrų vertės, parenkamas naujas centroidas esantis arčiausiai tikrojo grupės centro.
 Šie algoritmo žingsniai yra kartojami kol centroidai nebesikeičia \cite{Wu2008}. Atstumui įvertinti naudojamas Euklido atstumas \cite{comp}:
   \begin{equation}
   ||x_n - \mu_k||^2 = \sqrt{\sum_{i=1}^{D}(x_{ni} - \mu_{ki})^2}
   \end{equation}
   kur $\mu_k$ yra klasterio centroidas, $D$ yra duomenų rinkinio dydis, $x$ yra vienas iš duomenų rinkinio elementų.

   K vidurkių metodas turi trūkumų. Galutiniai klasteriai priklauso nuo pirminių taškų pasirinkimo. Netinkamai parinkti taškai gali lemti neoptimalų sprendimą. Taip pat k-vidurkių metodas nesugeba išskirti klasterių kurie nėra vienas nuo kito aiškiai atskirtos sferos erdvėje.

   Dėl vidurkio funkcijos naudojimo centroidų parinkimui rezultatas gali būti stipriai iškreiptas kelių išskirčių. Tai gali būti sprendžiama naudojant papildomą duomenų valymą, pirma suskaidant į daugiau klasterių, siekiant kad išskirtims būtų priskiriami maži klasteriai, ir vėliau jas prijungiant prie didesnių klasterių \cite{Wu2008}.

Affinity propagation yra vienas iš klasterizavimo metodų \cite{comp}. Šio metodo įvestis yra panašumų matrica, kurioje saugoma visų duomenų įrašų porų panašumai $s[i, j]$ kur $i, j =  (1,  2, ..., N)$. Naudojant šį metodą siekiama surasti kiekvieno elemento duomenų aibėje atstovą. Tą galima analizuoti kaip žinučių tarp duomenų įrašų perdavimą \cite{fastprop}. Kiekvienai elementų porai $i$ ir $j$ egzistuoja dvi žinutės -- atsakomybė (responsibility), $r[i, j]$ siunčiama iš taško $i$ į $j$, kuri parodo sukauptus įrodymus apie elemento $j$ galimybę atstovauti elementą $i$ klasteryje.  Antra žinutė yra tinkamumas (availability) $a[i, j]$, tai žinutė siunčiama iš elemento $j$ elementui $i$ kuri parodo $j$ elemento tinkamumą atstovauti elementą $i$. Iš pradžių visos $a$ ir $r$ reikšmės nustatomos į nulį ir yra iteratyviai atnaujinamos pagal \cite{fastprop}:
 \begin{equation}
    r[i, j] = (1 - \lambda)\rho[i, j] + \lambda r[i, j]
    a[i, j] = (1 - \lambda)\alpha[i, j] + \lambda a[i, j]
 \end{equation}
 kur $\lambda$ yra konstanta naudojama sumažinti svyravimus, galimos reikšmės yra $ 0 \geq \lambda < 1 $ , o $\rho[i, j]$ ir  $\alpha[i, j]$ yra propaguojama atsakomybė ir propaguojamas tinkamumas \cite{fastprop}. Šie dydžiai gaunami pagal \cite{fastprop}:
 \begin{equation}
   \rho[i, j]=\left\{
                  \begin{array}{l r}
                    s[i,j] - max_{k \neq j} \{ {a[i, k] + s[i, k]} \} & (i \neq j) \\
                    s[i,j] - max_{k \neq j} \{ {s[i, k]} \} & (i = j) \\
                  \end{array}
                \right.
 \end{equation}
   ir \cite{fastprop}:
\begin{equation}
    \alpha[i, j] =  \left\{
         \begin{array}{l r}
            min \{ 0, r[i, j] + \sum_{k \neq i,j} max \{ 0, r[k, j] \} \}         & (i \neq j) \\
            \sum_{k \neq i,j} max \{ 0, r[k, j] \}  & (i = j) \\
         \end{array}
       \right.
\end{equation}
žinutės yra gaunamos iš atitinkamų propaguojamų žinučių. Galiausiai atstovas elementui $i$ yra apibrėžiamas kaip \cite{fastprop}
\begin{equation}
    argmax \{r[i, j] + a[i, j] : j = 1, 2 ..., N \}
\end{equation}

Šis metodas leidžia kontroliuoti suformuojamų klasterių kiekį per $\lambda$ vertę \cite{comp}. Šis metodas pasižymi geresne greitaveika lyginant su K-vidurkių metodu \cite{fastprop}.

Pritaikant šiuos metodus svetainės turi būti supaprastinamos iki tam tikrų savybių vektoriaus. Tyrimo metu nauodtas svetainių turinys, jų URL adresai \cite{comp}. Svetainių turinys, kuris yra anglų kalba buvo analizuojamas siekiant išgauti jų semantinę prasmę. Tam naudota Term Frequency - inverse document frequency (TFIDF) metodika. Taip pat į svetainių savybes įtraukta ir jų HTML kodo struktūra, jų adreso semantinė reikšmė, nuorodos į kitus tinklalapius, atvaizduoto tinklalapio vaizdas.

Duomenų rinkinys surinktas iš dviejų pagrindinių šaltinių, Alexa katalogo, iš kurio išgautos nekenksmingos svetainės, bei iš Phishtank žalingų svetainių registro. Surinkta $100000$ svetainių informacija.

\ktufigure{images/validation.png}{11 cm}{Modelio apmokymo ir validacijos proceso schema}

Modeliai apmokyti naudojant 70\% pradinių, sužymėtų duomenų apmokymui bei 30\% testavimui \ktufigref{images/validation.png}. Duomenų rinkinys atsitiktinai skaidomas į dvi dalis, mokymo ir validacijos. Mokymo duomenys naudojami modelio apmokymui, o validacijos duomenys naudojami įvertinti modelį. Taip yra išvengiama modelio persimokymo, galima teisingai suderinti modelio hiperparametrus. Kenksmingų ir nekenksmingų svetainių santykis išlaikytas vienodas testavimo ir mokymo imtyse.

\begin{ktutable}{webtrees_rezultatas}{Strapsnyje pateikiami modelių rezultatai}
    \begin{tabular}{l c c c c }
     \hline
       \diagbox{Metrika}{Modelis} & KNN & LS & RS & NB \\ \hline
        Tikslumas, naudojant 50 įrašų & 74\% & 80\% & 79\% & 77\% \\ \hline
        Tikslumas, naudojant 100 įrašų & 75\% & 82\% & 83\% & 78\% \\ \hline
        Tikslumas, naudojant 500 įrašų & 79\% & 86\% & 92\% & 78\% \\ \hline
        Tikslumas, naudojant 5000 įrašų & 91\% & 93\% & 97\% & 84\% \\ \hline
        Tikslumas, naudojant 100,000 įrašų & 95\% & 93\% & 98\% & 89\% \\ \hline
        AUC                               & 0.66  &  0.93 & 0.91 & 0.88 \\ \hline
        kur modeliai: \\
        KNN - K-artimiausių kaimynų \\
        LS - Tiesinio branduolio atraminių vektorių klasifikatorius \\
        RS - Radial Basis Function branduolio atraminių vektorių klasifikatorius \\
        NB - Paprastasis Bayes klasifikatorius
    \end{tabular}
\end{ktutable}

Modelių rezultatai pateikiami \vref{tab:webtrees_rezultatas} lentelėje. Modelis pasiekęs geriausius rezultatus yra tiesinio branduolio atraminių vektorių klasifikatorius. Deja šiame straipsnyje nėra vertinami sprendimų medžiai, dėl to nėra aišku, koks būtų jų rezultatas naudojant šiuos duomenų ir savybių rinkinius.